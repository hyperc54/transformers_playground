{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b5b20f-7cf8-4123-808c-7213924feaff",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook builds a simple dataset which will be a base for our RAG system.\n",
    "The dataset will be composed of random wikipedia pages. It makes a good corpus to practice building\n",
    "RAGs for a common usecase: internal documentation question answering. There are of course a few differences, including\n",
    "scale (depending on the company) and lack of specialised lingo and concepts which are often out of\n",
    "the LLMs training distribution (Wikipedia is actually in pre-training corpus of many LLMs).\n",
    "\n",
    "Some chunking methods implemented can also be found in popular libraries (eg: LangChain). We are rewriting them for fun here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed33b7-dfcb-496b-b45b-4b88c1c598a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Callable, Iterable, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "LOCAL_DATASET_FOLDER = \"local_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7a24f-d2bb-42d7-9d5d-0048933556be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves some time to avoid fetching and parsing pages on your own\n",
    "# by loading a HF dataset of wikipedia pages\n",
    "wiki_data = load_dataset(\"wikipedia\", \"20220301.simple\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef683157-c153-41e3-ae23-396928456a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8ecd8-8d07-4fec-9fb8-89b1a052303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at one element\n",
    "first_el = wiki_data[0].copy() # for safe edits\n",
    "first_el[\"text\"] = first_el[\"text\"][:200] + \"...\" # for easier readability\n",
    "first_el"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a598a3-42a3-4059-b6a8-6ab92670be0f",
   "metadata": {},
   "source": [
    "We are pretty close to the dataset format we need for indexing. The main blocker is that the text field is too long for the limited context size of some embedding models we'd like to use (eg: BERT uses a context of ~512 token which is about ~512 words). We could also use\n",
    "larger models with larger context sizes, but research also suggest that models tend to lose track of some information in large context sizes: https://huggingface.co/papers/2307.03172.\n",
    "\n",
    "As a result, it seems favorable to keep using a relatively small context size -> We'll need to chunk our text examples. The rest of the notebook plays with different methods to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea6419-d38a-4211-8797-9cc4034bf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utils to apply a chunking method to the dataset per batch\n",
    "# We'll define different chunking methods to use after this\n",
    "def get_chunk_from_batch(\n",
    "    examples_batch: Iterable,\n",
    "    chunk_text_method: Callable,\n",
    "    **chunk_text_method_kwargs: Dict[str, int]\n",
    ") -> Dict[str, Iterable[Any]]:\n",
    "    \"\"\"\n",
    "    Apply 'chunk_text_method' to the examples_batch and returns\n",
    "    a dictionnary in the formatexpected by the Dataset.map method (Dict[str, Iterable[Features values]])\n",
    "    \"\"\"\n",
    "    example_ids = []\n",
    "    example_urls = []\n",
    "    example_titles = []\n",
    "    chunks = []\n",
    "    for ind, example_text in enumerate(examples_batch[\"text\"]):\n",
    "        for chunk in chunk_text_method(example_text, **chunk_text_method_kwargs):\n",
    "            example_ids.append(examples_batch[\"id\"][ind])\n",
    "            example_titles.append(examples_batch[\"title\"][ind])\n",
    "            example_urls.append(examples_batch[\"url\"][ind])\n",
    "            chunks.append(chunk)\n",
    "    return {\n",
    "        \"id\": list(range(len(chunks))),\n",
    "        \"original_id\": example_ids,\n",
    "        \"title\": example_titles,\n",
    "        \"url\": example_urls,\n",
    "        \"text_chunk\": chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15ff0b-912c-4909-a7a4-25bc191c2580",
   "metadata": {},
   "source": [
    "# Chunking strategies\n",
    "\n",
    "## (Dummy) Fixed-length chunking (with some overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b77410-1a31-4d8b-9412-44da5c3ccb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE_WORDS = 300\n",
    "OVERLAP_SIZE_WORDS = 10\n",
    "\n",
    "# Alternative to using LangChain methods\n",
    "# We do it indepedently of any tokeniser to make it generic (using words as a unit), at the risk\n",
    "# of having issues with model context size later on if the number of tokens in the chunk is too high\n",
    "def chunk_text_with_fixed_length(\n",
    "    text: str,\n",
    "    chunk_size_words: int = CHUNK_SIZE_WORDS,\n",
    "    overlap_size_words: int = OVERLAP_SIZE_WORDS\n",
    ") -> Iterable[str]:\n",
    "    text_no_new_lines = text.replace(\"\\n\", \" \")\n",
    "    text_split = text_no_new_lines.split(\" \")\n",
    "\n",
    "    total_words = len(text_split)\n",
    "    # iterate over words, chunk\n",
    "    word_index = 0\n",
    "    while word_index < total_words:\n",
    "        yield \" \".join(text_split[word_index:word_index+chunk_size_words])\n",
    "        word_index += chunk_size_words - overlap_size_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e22f2f-1365-41f5-b2b7-491fe7562900",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked = wiki_data.map(\n",
    "    lambda example_batch: get_chunk_from_batch(example_batch, chunk_text_with_fixed_length),\n",
    "    batched=True,\n",
    "    remove_columns=[\"id\", \"title\", \"text\", \"url\"] # Removes columns because of row expansion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1952d-305f-41e5-8ff4-9264afb0b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24789a01-5eba-44b6-8edb-d62f675287df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data back to disk\n",
    "FIXED_LENGTH_CHUNK_DATASET_NAME = f\"wiki-data-chunked-fixed-length-CS{CHUNK_SIZE_WORDS}-OS{OVERLAP_SIZE_WORDS}\"\n",
    "wiki_data_chunked.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, FIXED_LENGTH_CHUNK_DATASET_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3bcf6b-e656-4642-9138-cab5cd244b0f",
   "metadata": {},
   "source": [
    "\n",
    "## Paragraph recursive chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e8d02-dd2a-4a95-a668-7d50aa4ce860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse main sections and try to use those as chunks,\n",
    "# Sections that are too long are split by sub-sections, and the same logic is applied recursively\n",
    "# Parsing sections is done differently depending on the document format. For markdown, we'd split on '#' then '##' etc.\n",
    "# with this corpus, paragraphs and sections are split with '\\n\\n' and it's hard to infer sections titles besides checking the size of the section\n",
    "# We can simply assume that '\\n\\n' represent relatively good semantic breaks, and recursively use those to break sections that are too long in 'half'\n",
    "\n",
    "# Makes the assumption that individual paragraphs are all smaller than section size\n",
    "\n",
    "# Prepend all sections with title and subtitle\n",
    "# cut on titles, looks at section sizes, if too long, cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d32b7-6e73-43b3-9d53-483b1727a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_recursively_per_section(text: str, max_chunk_size_words: int = CHUNK_SIZE_WORDS) -> Iterable[str]:\n",
    "    SPLIT_STR = \"\\n\\n\"\n",
    "    text_split = text.split(\" \")\n",
    "    if len(text_split) > max_chunk_size_words and SPLIT_STR not in text:\n",
    "        # We can't split the text further and it's too big, resolve to dummy chunking strategy\n",
    "        return list(chunk_text_with_fixed_length(text, max_chunk_size_words, OVERLAP_SIZE_WORDS))\n",
    "    elif len(text_split) <= max_chunk_size_words:\n",
    "        return [text]\n",
    "    else:\n",
    "        # There's at least one split candidate in the text, pick the best one\n",
    "        # (=the one that looks to be the closest to the middle)\n",
    "        text_len = len(text)\n",
    "        all_potential_splits = [m.start() for m in re.finditer(SPLIT_STR, text)]\n",
    "        all_potential_splits_distances_to_half = [abs(split_ind - text_len//2) for split_ind in all_potential_splits]\n",
    "        best_split_ind = all_potential_splits[all_potential_splits_distances_to_half.index(min(all_potential_splits_distances_to_half))]\n",
    "\n",
    "        return chunk_text_recursively_per_section(text[0:best_split_ind], max_chunk_size_words) + chunk_text_recursively_per_section(text[(best_split_ind+len(SPLIT_STR)):], max_chunk_size_words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aded6d-1392-4335-aa2e-8541b9516c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked_recursive = wiki_data.map(\n",
    "    lambda example_batch: get_chunk_from_batch(example_batch, chunk_text_recursively_per_section),\n",
    "    batched=True,\n",
    "    remove_columns=[\"id\", \"title\", \"text\", \"url\"] # Removes columns because of row expansion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d01f75-73ae-430f-9fac-4af5071b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1faf7c9-6dd2-44ad-907e-4e890dd887c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data back to disk\n",
    "RECURSIVE_DATASET_NAME = f\"wiki-data-chunked-recursive-CS{CHUNK_SIZE_WORDS}\"\n",
    "wiki_data_chunked_recursive.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, RECURSIVE_DATASET_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b65596d-e04d-428b-8f5b-5f5c5ff831cc",
   "metadata": {},
   "source": [
    "## [Optional - can be skipped] Modelling approach\n",
    "\n",
    "If the previous method yields disappointing results, which could happen if the segmentation of sections is harder to work with, we could use a slightly more esoteric approach using a language model to detects interesting splitting points.\n",
    "\n",
    "This could be done in different ways which may have varying performance depending on the dataset, a few similar ideas include:\n",
    "    - Simply ask an LLM for the split points\n",
    "    - Use an embedding model to capture the semantic meaning of each sentence, and add a split point where the topic seems to shift significantly\n",
    "    - Use a model trained on 'Next Sentence Prediction' and add a split point where the model confidently says sentences are disconnected.\n",
    "\n",
    "We'll try the latter here with [BERT](https://huggingface.co/google-bert/bert-base-uncased). #TODO replace with MiniLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644e126-a39e-4cad-9546-5ee2969ac732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3fa18-bc33-4bd7-bf71-7a2a390f0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sentence_next(bert_model, bert_tokeniser, sentence_a, sentence_b):\n",
    "    encoding = bert_tokeniser(sentence_a, sentence_b, return_tensors=\"pt\")\n",
    "    outputs = bert_model(**encoding, labels=torch.LongTensor([1]))\n",
    "\n",
    "    # Decision logic to decide if we'd like to break the chunk here.\n",
    "    # Finding the right threshold/logic requires some trials and errors and probably depends on the dataset used\n",
    "    return outputs.logits[0, 0] > outputs.logits[0, 1] # Same sentence more likely than random\n",
    "        \n",
    "\n",
    "def chunk_text_with_bert(text: str, max_chunk_size_words: int = CHUNK_SIZE_WORDS):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "    model.eval()\n",
    "    model.to(\"cpu\") # switch to CUDA if you have a GPU\n",
    "\n",
    "    # Split per sentence\n",
    "    text_sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Make sure sentences are all small enough to be sent forward. A dummy approach is to\n",
    "    # truncate those.\n",
    "    text_sentences = [\n",
    "        \" \".join(sentence.split(\" \")[:int(0.8*max_chunk_size_words)])\n",
    "        for sentence in text_sentences\n",
    "    ]\n",
    "    \n",
    "    chunks = []\n",
    "    last_sentence = text_sentences[0]\n",
    "    current_chunk = text_sentences[0]\n",
    "\n",
    "    for sentence in text_sentences:\n",
    "        current_chunk_size_words = len(current_chunk.split(\" \"))\n",
    "        sentence_size_words = len(sentence.split(\" \"))\n",
    "        if current_chunk_size_words + sentence_size_words > max_chunk_size_words:\n",
    "            # we have to split in any case\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        if is_sentence_next(model, tokenizer, last_sentence, sentence):\n",
    "            # add to current chunk and continue\n",
    "            current_chunk = current_chunk + sentence\n",
    "        else:\n",
    "            # split\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        last_sentence = sentence\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25902a-8b4e-465f-8f24-c520b56bd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method is quite slow and would benefit optimisation! for learning purpose only\n",
    "wiki_data_chunked_w_model = wiki_data.select(range(200)).map(\n",
    "    lambda example_batch: get_chunk_from_batch(example_batch, chunk_text_with_bert),\n",
    "    batched=True,\n",
    "    remove_columns=[\"id\", \"title\", \"text\", \"url\"], # Removes columns because of row expansion\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2f634-de6d-4d51-adbb-7715c13958e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked_w_model[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad74804f-cc09-4124-9a49-496b667e009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data back to disk\n",
    "MODEL_DATASET_NAME = f\"wiki-data-chunked-w-model-CS{CHUNK_SIZE_WORDS}\"\n",
    "wiki_data_chunked_w_model.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, MODEL_DATASET_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e641f-6734-42bb-9470-a1fb417a6cc7",
   "metadata": {},
   "source": [
    "# Questions-Answers generation\n",
    "It would be excellent to have human-curated questions answers pairs to evaluate our retrieval logic.\n",
    "(a bit like in https://www.kaggle.com/datasets/rtatman/questionanswer-dataset?resource=download)\n",
    "\n",
    "If we can't afford this, we can always generate questions/answers with an LLM as well.\n",
    "\n",
    "The notebook implements both methods below, if you have time on your hands to annotate things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e5142-bd3d-418b-a329-c59a1836ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_add_questions_to = RECURSIVE_DATASET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59572ce0-56e6-47ab-9d0f-4f24dc470a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked = load_from_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, dataset_to_add_questions_to)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8e2bf-5dbe-4e34-b5fd-65a01b711b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data_chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d644c2-42a3-4832-85b7-6ab82669780b",
   "metadata": {},
   "source": [
    "## Manual hand-labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34693720-0315-4839-b4fc-b0b584f07810",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset_name = f\"{dataset_to_add_questions_to}-questions\"\n",
    "\n",
    "data = []\n",
    "while True: # Interrupt when you'd like to stop\n",
    "    rnd_chunk_id = np.random.randint(len(wiki_data_chunked))\n",
    "    print(\"-----------------------\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"New chunk to annotate!\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"-----------------------\")\n",
    "    print(wiki_data_chunked[rnd_chunk_id][\"text_chunk\"])\n",
    "    question = input(\"Type a question:\")\n",
    "    answer = input(\"Type the answer to the question:\")\n",
    "\n",
    "    new_el = {\n",
    "        \"chunk_id\": rnd_chunk_id,\n",
    "        \"question\": question,\n",
    "        \"answer\" : answer\n",
    "    }\n",
    "    data.append(new_el)\n",
    "    \n",
    "    print(f\"New data point: {new_el}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541da625-3487-44c7-8cb8-5a6ad8a2a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2444e6f-23b3-47a8-982b-9faed9a4fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aafaeb-bb33-4575-9138-bde50b9e8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, questions_dataset_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff58e2-8f4a-4480-9b54-1dccf76a6a85",
   "metadata": {},
   "source": [
    "## LLM labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d2354-b110-4177-81c0-f2582d612daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fun, we can tryout the HF inference API\n",
    "# os.environ[\"HF_TOKEN_SERVERLESS_API\"] = \"hf_*\"\n",
    "token = os.environ[\"HF_TOKEN_SERVERLESS_API\"] # ADD YOUR TOKEN TO YOUR ENV! (It's a free service)\n",
    "client = InferenceClient(\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329c0e6-d753-4735-9f97-3989e1d35e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_question_pair_from_llm(text):\n",
    "    response = client.chat_completion(\n",
    "    \tmodel=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \tmessages=[\n",
    "            # Prompt can be improved, LLM sometimes outputs things like \"Who are notable figures mentioned in this list?\"\n",
    "            # which obviously doesnt work as we won't have access to the list... What would you suggest we change?\n",
    "            {\"role\": \"user\", \"content\": \"You are a helpful assistant. You will receive text chunks in quotes from users that originate from a wikipedia page. Your task will be to create a question/answer pair from this text chunk, with the answer being present in the chunk. Answer the query in the form [question] END_QUESTION [answer], nothing more. Please write short questions!\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Sure! understood.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"'{text}'\"}],\n",
    "    \tmax_tokens=50,\n",
    "    )\n",
    "\n",
    "    llm_output = response.choices[0][\"message\"][\"content\"]\n",
    "    \n",
    "    result = llm_output.split(\" END_QUESTION \")\n",
    "    if len(result.split(\" END_QUESTION \")) != 2:\n",
    "        print(\"LLM Failed! returning None\")\n",
    "        return None, None\n",
    "    \n",
    "    question, answer = result\n",
    "    return question, answer\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27662cee-f939-4e00-8630-4128b3a4f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it once before sending multiple requests\n",
    "fetch_question_pair_from_llm(\n",
    "    wiki_data_chunked[0][\"text_chunk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445ad91-4f59-42b1-9505-747fa754436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REQUESTS = 100\n",
    "\n",
    "data = []\n",
    "for _ in tqdm(range(N_REQUESTS)):\n",
    "    rnd_chunk_id = np.random.randint(len(wiki_data_chunked))\n",
    "    text_chunk = wiki_data_chunked[rnd_chunk_id][\"text_chunk\"]\n",
    "    print(\"--- New ELEMENT ---\")\n",
    "    print(\"Fetching a question for:\")\n",
    "    print(text_chunk)\n",
    "    print(\"LLM answered:\")\n",
    "\n",
    "    question, answer = fetch_question_pair_from_llm(text_chunk)\n",
    "    \n",
    "    new_el = {\n",
    "        \"chunk_id\": rnd_chunk_id,\n",
    "        \"question\": question,\n",
    "        \"answer\" : answer\n",
    "    }\n",
    "    data.append(new_el)\n",
    "    \n",
    "    print(f\"New data point: {new_el}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769b4e2-bfbc-46c2-a4d0-949a3eeca8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset_llm = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985cab47-5b40-4fd5-87c1-91e790803622",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_dataset_llm_name = f\"{dataset_to_add_questions_to}-questions-llm\"\n",
    "\n",
    "questions_dataset_llm.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, questions_dataset_llm_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513bd33-2c60-482f-af82-012fbba4ac03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rak_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
