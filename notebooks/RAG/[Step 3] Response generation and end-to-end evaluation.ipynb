{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93212ba6-0ce9-4f1a-b134-1c22aff8d803",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook is part of a mini-series that aims to build a simple RAG system for learning purposes.\n",
    "\n",
    "This notebook assumes:\n",
    "- we have an index of embeddings representing documents\n",
    "- We have a dataset of questions which can be answered by looking at information in specific chunk_ids (to simplify, we'll assume there's only one relevant chunk_id), **with embeddings as well that match target documents** `question (str) | answer (str) | relevant_chunk_id (int) | embedding (tensor(N))`\n",
    "\n",
    "We'd like now from a question (that we'd manually input or from the dataset of questions) to generate an answer back to\n",
    "the user by providing the matching documents as context to an LLM.\n",
    "\n",
    "Plan:\n",
    "- The first section implements the end-to-end flow to manually play with the system\n",
    "- The second section runs the questions dataset through the flow to evaluate the end-to-end system. We'll evaluate the system in two ways:\n",
    "  1) With exact metrics.\n",
    "  2) With an evaluation made by a different LLM.\n",
    "- The third section will try to tune a few parameters to improve the evaluation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4917e-ca1d-4a46-8963-f1494ab31609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import InferenceClient\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import faiss\n",
    "\n",
    "LOCAL_DATASET_FOLDER = \"local_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ff8d35-1431-4ea5-9d81-1c74e785e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DATASET_NAME = \"wiki-data-chunked-recursive-CS300\"\n",
    "QUESTIONS_DATASET_NAME = \"wiki-data-chunked-recursive-CS300-questions-llm\"\n",
    "\n",
    "# We are loading the corpus to retrieve the actual chunks to add as context in\n",
    "# the response generator model. In a production setting, the chunk would probably\n",
    "# be retrieved from the index directly.\n",
    "ds_corpus = load_from_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, CORPUS_DATASET_NAME)\n",
    ")\n",
    "\n",
    "# Index contains embeddings and ids for al chunks in the corpus, but not the content of those chunks themselves\n",
    "index = faiss.read_index(os.path.join(LOCAL_DATASET_FOLDER, f\"flat_index_{CORPUS_DATASET_NAME}.index\"))\n",
    "\n",
    "# Questions dataset used for evaluation\n",
    "ds_qas = load_from_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, QUESTIONS_DATASET_NAME + \"with_simple_embeddings\")\n",
    ")\n",
    "\n",
    "# The questions dataset has embeddings ready to use, but if you'd life to create new embeddings, load the retriever\n",
    "# model as well\n",
    "retriever_emb_model = SentenceTransformer(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, f\"embedding_model_{CORPUS_DATASET_NAME}.model\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff7a1d-494b-49a6-af6b-17149055b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On text generation, are we trying to get conversational? creative? (in that case we might want to go for a causal modeling approach)\n",
    "# or rather simply extract the answer from the context? (in that case we might want to got for a QuestionAnswering trained model approach)\n",
    "\n",
    "# For the purpose of this exercise, we'll go with the causal modeling approach, but it really depends on the end product\n",
    "# the one you would prefer to use\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "device = \"cpu\" # Speed of small models on CPU is pretty cool\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7390d6-da24-4e45-8e57-04554ce440ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the maximum context size we'll be able to put in the model prompt\n",
    "# 8k tokens should be completely fine for this small project\n",
    "# as we tried to have text_chunks with fewer than 300 words in a previous notebook\n",
    "# but more ambitious projects will probably require bigger context sizes\n",
    "generator_model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf572e8b-9b9e-4655-a9a8-139b1fb3fae1",
   "metadata": {},
   "source": [
    "# End-to-End response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc191220-3283-4cce-844b-0a8f676b28a7",
   "metadata": {},
   "source": [
    "## Test with random user questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba24f7a-9b39-4904-b43a-f9b17495260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method holds the end-to-end processing.\n",
    "# it could be exported to a standalone app\n",
    "def generate_response(question: str, number_of_chunks_in_context:int = 1):\n",
    "    # generate question embedding\n",
    "    embedding = torch.Tensor(retriever_emb_model.encode([question]))\n",
    "    \n",
    "    # query the index\n",
    "    distances, indices = index.search(embedding, k=number_of_chunks_in_context)\n",
    "\n",
    "    # retrieve the relevant chunk ids\n",
    "    text_chunks = ds_corpus.select(indices[0, :])[\"text_chunk\"]\n",
    "\n",
    "    # build a prompt for the response generator model\n",
    "    system_prompt = \"You are a helpful assistant that will answer users questions. Before their question, users will give you some text content as context that will help you answer the questions. The answer to their question should be inside the context. Please answer accurately and concisely.\"\n",
    "    user_prompt = \"\"\n",
    "    for i, text_chunk in enumerate(text_chunks):\n",
    "        user_prompt += f\"[Context #{i}] {text_chunk}.\"\n",
    "    user_prompt += f\"[Question] {question}\"\n",
    "\n",
    "    # generate response from prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    input_tokens=tokenizer.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = generator_model.generate(\n",
    "        input_tokens,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.2, # [Exercise] how does this influence this answer? How would you tune it?\n",
    "        top_p=0.9, # [Exercise] how does this influence this answer? How would you tune it?\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Parse output, need to manually skip the 'assistant' token added which is not special (+4)\n",
    "    answer = tokenizer.decode(outputs[0, len(input_tokens[0])+4:], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Running it a few times, we notice that the answer is sometimes truncated -> we are overflowing the context\n",
    "# size! -> [Exercise] What would you suggest to improve this\n",
    "generate_response(\"Who is Abraham Lincoln?\", number_of_chunks_in_context=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a2848-233c-4091-95a3-404d982318f1",
   "metadata": {},
   "source": [
    "## Test on our questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612806f-5f68-4594-862c-2346322d7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers from our e2e system for our questions dataset\n",
    "# Map method could benefit from batching, but fortunately our dataset is pretty small\n",
    "ds_qas = ds_qas.map(\n",
    "    lambda element: {\"rag_answer\": generate_response(element[\"question\"], number_of_chunks_in_context=1)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9bb9d-469f-4010-9ce0-6c482b971612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eye-ball a few question/answer/rag_answer triplets\n",
    "for i in range(10):\n",
    "    print(\"--\")\n",
    "    print(ds_qas.select_columns([\"question\", \"answer\", \"rag_answer\"])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548dea2-13a3-47fc-b892-1b84d44a91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like we have some wins and some misses! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8fa760-2dec-435f-bba7-fefa6d988a91",
   "metadata": {},
   "source": [
    "# End-to-End Evaluation\n",
    "\n",
    "We are looking into automatically evaluating the quality of our answer here (without human involvment) for the end-to-end system (question embedding -> index retrieval -> answer generation).\n",
    "We will reuse our dataset of questions which already contain answers that we can compare our output to.\n",
    "\n",
    "What are we trying to measure in the answer we generate? we'd like to know if:\n",
    "- it contains the answer to the question as it was written in the groundtruth. (recall)\n",
    "- not much else, unless we want the answer to be pretty conversational/welcome more details. (precision)\n",
    "\n",
    "\n",
    "We will then compare our answer to the groundtruth via 3 methods:\n",
    "- Exact metrics (eg: ROUGE) which apply simple methods to rate the answer\n",
    "- 'LLM as a judge' metrics which use models to rate the answer\n",
    "- Semantic similarity which use models to compare the semantic meaning of the answer provided vs groundtruth\n",
    "\n",
    "All methods have their pros and cons and are complimentary of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905fd26d-6a60-4a89-8659-42ae94115dcb",
   "metadata": {},
   "source": [
    "## Exact Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff30661f-6dcb-4e6c-8d2a-8244119a07e3",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "To evaluate Recall we'd like to know if our answer contains the most relevant 'keywords' that are also in the groundtruth.\n",
    "A popular metric that does this is ROUGE (R stands for Recall!). It compares the overlap between the answer provided and the groundtruth, through n-grams (so, not only looking at keywords but also pair of words etc.), and computes the % of expected-n grams that appeared in our response. For simplicity here we'll only look at uni-grams, but looking at more metrics is always useful.\n",
    "\n",
    "The higher the better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9175c-fae8-4b83-a729-9e534ad1137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take unigrams\n",
    "r_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa44cf9-0992-446e-b402-e159b3f370cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get better intution on the metrics, display a few answers with their respective ROUGE score\n",
    "for i in range(5):\n",
    "    print(\"--\")\n",
    "    print(f\"Groundtruth: {ds_qas['answer'][i]}\")\n",
    "    print(f\"RAG Prediction: {ds_qas['rag_answer'][i]}\")\n",
    "    print(f\"ROUGE score individual predictions: {r_scorer.score(target=ds_qas['answer'][i], prediction=ds_qas['rag_answer'][i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75a89c-9a54-4ca8-835d-554c03ddedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE recall is able to flag clear misses... but also gives pretty low score to a perfectly good enough answer\n",
    "# simply because the groundtruth says 'and many others' when the generated answer lists all relevant tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d39313-6155-4adf-8e77-3031c7c0f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the ROUGE package also outputs a _precision_ metric!\n",
    "\n",
    "# it's the % of n-grams that we output in our answer that is also in the groundtruth. Obviously, the more irrelevant tokens\n",
    "# we add, the lower this goes (as opposed to recall, in which adding irrelevant tokens doesn't change the metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d623792-7542-4a9f-8b27-4c7d6e178439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add individual ROUGE metrics\n",
    "def compute_rouge_metrics(ds_qa_element: Dict[str, Any]):\n",
    "    r_metrics = r_scorer.score(\n",
    "        target=ds_qa_element['answer'],\n",
    "        prediction=ds_qa_element['rag_answer']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge_recall\": r_metrics[\"rouge1\"].recall,\n",
    "        \"rouge_precision\": r_metrics[\"rouge1\"].precision\n",
    "    }\n",
    "\n",
    "ds_qas = ds_qas.map(compute_rouge_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c781f8e-13b2-426a-8acb-c9e1e1dded31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean ROUGE Recall: {np.mean(ds_qas['rouge_recall'])}\")\n",
    "print(f\"Mean ROUGE Precision: {np.mean(ds_qas['rouge_precision'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492170c-8e25-452a-8a9b-284f5c984d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This could suggest we manage to retrieve relevant information ~ half of the time but add too many things\n",
    "# to the answer\n",
    "# [Exercise] How would you tune our system to be better on ROUGE Precision? Ask the generator to be even more concise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a87fb-3d75-4d22-8ecb-be11c0e78a79",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Unless we are interested in additional details, we'd like here to know if our model is as concise as possible.\n",
    "If we assume the answers in our groundtruth to be concise. A really straightforward (but not without flaws) \n",
    "to measure 'Precision' is to compare the length of our generated answer to the groundtruth, and assume that\n",
    "answers that are way too long are likely to lack precision.\n",
    "\n",
    "Another popular metric is BLEU, but it's computation is 'similar' to the ROUGE precision we've computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edc612-94dd-4480-9d17-3224654c2a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_qas = ds_qas.map(\n",
    "    lambda element: {\"length_diff_words_answers\": len(element[\"rag_answer\"].split(\" \")) - len(element[\"answer\"].split(\" \"))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26de99-5a37-41e9-8e49-e52a850eef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_answer_shorter_than_gt = len(ds_qas.filter(lambda el: el[\"length_diff_words_answers\"] < 0)) / len(ds_qas)\n",
    "print(f\"Our answer is shorter than the groundtruth {100*ratio_answer_shorter_than_gt:.2f}% of the time.\")\n",
    "\n",
    "mean_length_longer_answers = np.mean(\n",
    "    ds_qas.filter(\n",
    "        lambda el: el[\"length_diff_words_answers\"] > 0\n",
    "    )[\"length_diff_words_answers\"]\n",
    ")\n",
    "print(f\"When our answer is longer, it has on average {mean_length_longer_answers} more words than the GT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df2542-258f-4ecc-8a75-77bcbeb45eda",
   "metadata": {},
   "source": [
    "## LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709677b9-57ca-420c-b274-1dbf8fa3b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience (and because our dataset is small), let's do it with HF Inference APi\n",
    "token = os.environ[\"HF_TOKEN_SERVERLESS_API\"] # ADD YOUR TOKEN TO YOUR ENV! (It's a free service)\n",
    "client = InferenceClient(\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba0124-646a-45e1-a386-0252bd250070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simply ask the LLM whether they think the answers are equivalent\n",
    "def ask_llm_opinion(question, answer, rag_answer):\n",
    "    response = client.chat_completion(\n",
    "    \tmodel=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \tmessages=[\n",
    "            # Prompt can be improved, LLM sometimes outputs things like \"Who are notable figures mentioned in this list?\"\n",
    "            # which obviously doesnt work as we won't have access to the list... What would you suggest we change?\n",
    "            {\"role\": \"user\", \"content\": \"You are a helpful assistant. You will receive a question and its answer. You will then receive an alternative answer for the same question and need to determine if the alternative answer contains the same facts as the orignal answer. You can ignore other details. Please only answer with yes or no.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Sure! understood.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Does the alternative answer contains the same fact as the original answer? Here is the triplet: [QUESTION] {question} [ANSWER] {answer} [ALTERNATIVE ANSWER] {rag_answer}\"}],\n",
    "    \tmax_tokens=4,\n",
    "    )\n",
    "\n",
    "    llm_output = response.choices[0][\"message\"][\"content\"]\n",
    "    \n",
    "    if \"yes\" in llm_output.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False # All the other answer will map to False!\n",
    "\n",
    "ask_llm_opinion(\n",
    "    question=\"What is 10+10?\",\n",
    "    answer=\"20\",\n",
    "    rag_answer=\"The answer is 20\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34175d5b-163e-4480-a634-8943eced4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_qas = ds_qas.map(\n",
    "    lambda element: {\n",
    "        \"llm_judge_answers_equivalent_opinion\": ask_llm_opinion(\n",
    "            question=element[\"question\"],\n",
    "            answer=element[\"answer\"],\n",
    "            rag_answer=element[\"rag_answer\"]\n",
    "        )\n",
    "    }\n",
    ")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d23e46-6f9f-4e80-bf12-f32652181456",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = len(ds_qas.filter(lambda el: el[\"llm_judge_answers_equivalent_opinion\"]))\n",
    "\n",
    "print(f\"The Judge LLM thinks RAG answers contain the relevant facts {(100* n_correct / len(ds_qas)):.2f}% of the time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9a66f-090b-4d02-98ff-e72409d5bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very close to our ROUGE Recall value! :yay:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584d1da-aaf7-40f7-8779-c7ba8ac780c6",
   "metadata": {},
   "source": [
    "## Sentence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041be71f-fda8-4290-b1a1-1211aa85287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left as an [Exercise]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8cbc88-5d60-43d1-b67d-f957db358748",
   "metadata": {},
   "source": [
    "# Specific response generation evaluation metrics\n",
    "Additionally, we could also evaluate the quality of the response generation specifically.\n",
    "That is, assuming the prompt contains accurate context, how likely are we to generate a good answer\n",
    "in return? (Assessing Recall = we are retrieving all the relevant answers IN THE CONTEXT, and Precision = we are only returning\n",
    "relevant answers)\n",
    "\n",
    "To do this we would need to build a dataset that contains with certainty the right context for the question (which is\n",
    "'probably' the case if we use the 'question' / 'text_chunk' from the ds_qas variable but not necessarily, we are talking\n",
    "about an LLM produced dataset here...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfe059-38c5-4f25-964b-ff067cbe791f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4b281-3efb-4220-bbb3-64017408c3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rak_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
