{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78360e6-8dfd-4772-b140-4f46d43e016f",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook is part of a mini-series that aims to build a simple RAG system for learning purposes.\n",
    "\n",
    "This notebook assumes:\n",
    "- we have a dataset available of chunks of text data to index `chunk_id (int) | text_chunk (str)`.\n",
    "- Optionally, (but really importantly, if we'd like to go beyond the first section and evaluate our retriever model), it also assumes we have a dataset of questions which can be answered by looking at information in specific chunk_ids (to simplify, we'll assume there's only one relevant chunk_id) in the form `question (str) | answer (str) | relevant_chunk_id (int)`\n",
    "\n",
    "Plan:\n",
    "- The first section loads an embedder model and maps it to the dataset of text chunks.\n",
    "- The second section evaluates the (dummy) solution explored in the first section using the dataset of questions.\n",
    "- The third section tries to improve the solution explored in the first section by fine-tuning our model on the questions dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f2715b-2630-4fd9-b2fe-52a5ef676fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "\n",
    "\n",
    "import faiss\n",
    "\n",
    "LOCAL_DATASET_FOLDER = \"local_datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a58c2d-f63c-4f9a-b2ee-7ac297ff2ec8",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a288a6-cddd-429b-ae65-358feb22ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DATASET_NAME = \"wiki-data-chunked-recursive-CS300\"\n",
    "QUESTIONS_DATASET_NAME = \"wiki-data-chunked-recursive-CS300-questions-llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fa79b2-196b-46c1-9df7-af5a5aa468e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset was created in the previous notebook\n",
    "# Contains:\n",
    "#  A: An extract of some english wikipedia articles\n",
    "#  B: A list of crowdsourced questions/answers pairs and the relevant article that contains the answer.\n",
    "ds_corpus = load_from_disk(os.path.join(LOCAL_DATASET_FOLDER, CORPUS_DATASET_NAME))\n",
    "ds_qas = load_from_disk(os.path.join(LOCAL_DATASET_FOLDER, QUESTIONS_DATASET_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7101550-58a8-48fb-8d80-698519edee8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'original_id', 'text_chunk'],\n",
       "    num_rows: 271938\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ab0734-bff4-46f7-bb0e-1c8b3de20d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunk_id', 'question', 'answer'],\n",
       "    num_rows: 96\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_qas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4112f9e-c47f-4be9-a08b-f7bee42ff5aa",
   "metadata": {},
   "source": [
    "# Section 1: Dummy straightforward solution\n",
    "(i.e map an embedding model over the whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66eda211-dfdd-4385-8fcd-cbb76aaa7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notebook series uses the miniLM model to create embeddings for questions.\n",
    "# However, in this notebook, we show how we could have done it with BERT as well.\n",
    "# (the other notebooks will expect MiniLM embeddings though)\n",
    "def retrieve_bert_embedding(bert_model, text_batch):\n",
    "    # We operate one text input at the time for simplicity.\n",
    "    # Using batches would be faster, but this implies padding inputs so they are all the same size and using attention masks properly.\n",
    "    encoded_inputs = tokenizer(text_batch, return_tensors='pt')\n",
    "\n",
    "    # The input needs to be < 512 tokens long otherwise the model call will crash\n",
    "    output = bert_model(**encoded_inputs)\n",
    "\n",
    "    # We use the pooler_output as embedding layer. It contains an embedding obtained for the [CLS] token added at\n",
    "    # the beginning.\n",
    "    # An alternative would be to retrieve multiple hidden embedding states and use those instead (by concatenating multiple layers),\n",
    "    # it might contain more semantic meaning at the expense of being a bigger vector. I would recommend to try both.\n",
    "    return torch.nn.functional.normalize(output.pooler_output) # Normalise as this will help for indexing, and doesn't change Cosine similarities\n",
    "\n",
    "def retrieve_minilm_embedding(mini_lm_model, text_batch):\n",
    "    embedding = torch.Tensor(mini_lm_model.encode(text_batch))\n",
    "\n",
    "    return embedding # MiniLM embeddings are already normalised\n",
    "\n",
    "# [Exercise]: Feel free to implement a different embedding retrieval method!\n",
    "# Some inspiration of Embedding models to use: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02272258-1152-4c0d-bf6b-3a7106f32a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_use = \"MiniLM\" # you can replace with \"BERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b098de6-9624-4614-b0ea-dbe102e83832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierret/.pyenv/versions/3.10.1/envs/transformers_playground/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if model_to_use == \"MiniLM\":\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    retrieve_embedding_method = retrieve_minilm_embedding\n",
    "elif model_to_use == \"BERT\":\n",
    "    # More info on BERT: https://huggingface.co/blog/bert-101\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    retrieve_embedding_method = retrieve_bert_embedding\n",
    "\n",
    "model.eval()\n",
    "    \n",
    "# switch to GPU if you have one available\n",
    "device = \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bdc136-c22a-4e92-86af-17bbb3facbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"BERT\":\n",
    "    # Play with the tokeniser a bit, note the added tokens and the casing dropped\n",
    "    # Always check intermediate inputs!\n",
    "    tokenizer.decode(tokenizer(\"Hi! nice to meet you\", return_tensors='pt')[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772f705d-9ab4-4d80-986a-caf96e502aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim(embedding_food, embedding_food2)=tensor(0.5597)\n",
      "cos_sim(embedding_food, embedding_history_lesson)=tensor(0.1039)\n",
      "cos_sim(embedding_food, embedding_fra)=tensor(0.0042)\n",
      "cos_sim(embedding_food, torch.randn(embedding_food.shape[0]))=tensor(0.0327)\n",
      "cos_sim(embedding_food, embedding_trap)=tensor(0.2990)\n"
     ]
    }
   ],
   "source": [
    "# quickly check that we can capture semantic similarity\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "embedding_food = retrieve_embedding_method(model, \"I like to eat ice creams and donuts.\")\n",
    "embedding_food2 = retrieve_embedding_method(model, \"My favourite food is gelato, and sugar pastries.\")\n",
    "embedding_history_lesson = retrieve_embedding_method(model, \"Napoleon III was a popular monarch who oversaw the modernization of the French economy.\")\n",
    "embedding_fra = retrieve_embedding_method(model, \"BERT n'a pas été entrainé avec du Français.\")\n",
    "embedding_trap = retrieve_embedding_method(model, \"This road is shaped like a donut.\") # Contains the same word in a different context\n",
    "\n",
    "print(f\"{cos_sim(embedding_food, embedding_food2)=}\")\n",
    "print(f\"{cos_sim(embedding_food, embedding_history_lesson)=}\")\n",
    "print(f\"{cos_sim(embedding_food, embedding_fra)=}\")\n",
    "print(f\"{cos_sim(embedding_food, torch.randn(embedding_food.shape[0]))=}\")\n",
    "print(f\"{cos_sim(embedding_food, embedding_trap)=}\")\n",
    "# Some results might be a bit surprising for some models, but at least the most similar vectors are the expected ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1bc86ff-fc9c-413e-af45-9f77993b17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_corpus = ds_corpus.map(\n",
    "    lambda element: {\"embedding\": retrieve_embedding_method(model, element[\"text_chunk\"])},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9cebe1c-43ec-472b-8919-15f5488761c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_qas = ds_qas.map(\n",
    "    lambda element: {\"embedding\": retrieve_embedding_method(model, element[\"question\"])},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "781bbc15-ef9b-434a-9788-b335babf7fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb778202397494d8cc50fabcb0b43d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/271938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae95e1bc776b4d54b03219a1fb6b90ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And that's already done! We can save it for indexing later on...But how good are these embeddings really?\n",
    "# We'll investigate this now\n",
    "ds_corpus.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, CORPUS_DATASET_NAME + \"with_simple_embeddings\")\n",
    ")\n",
    "\n",
    "ds_qas.save_to_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, QUESTIONS_DATASET_NAME + \"with_simple_embeddings\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f9723-9ea5-422d-a032-e584495642f0",
   "metadata": {},
   "source": [
    "# Section 2: Evaluation\n",
    "We'd like to know if our embeddings capture the semantic meaning of passages well enough so that,\n",
    "for each question/answer pair in our questions dataset, we can retrieve the right document\n",
    "by embedding the question input and running a nearest-neighbour search over the dataset... BUT\n",
    "this implies we have our indexing solution ready to use already.\n",
    "\n",
    "We'll try:\n",
    "- to evaluate the quality of the embeddings without actually running a kNN search\n",
    "- evaluate the whole retrieval process by using a very simple kNN search method (which works here because the dataset is really small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c6e1c2a-9b50-4d79-90a0-fa3885360dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally reload datasets previously checkpointed\n",
    "ds_corpus = load_from_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, CORPUS_DATASET_NAME + \"with_simple_embeddings\")\n",
    ")\n",
    "\n",
    "ds_qas = load_from_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, QUESTIONS_DATASET_NAME + \"with_simple_embeddings\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a3126-4bb8-4f59-aad5-c036011b6840",
   "metadata": {},
   "source": [
    "## A) Quality of embeddings without kNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f10fb4d1-0fa0-4156-9d00-5aed4c09ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b898e59e799744188d334cb6b16106f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each question,\n",
    "# without running an actual kNN search, we can simply compare the cosine similarity between:\n",
    "# - the question embedding / target document embedding\n",
    "# - the question embedding / other random documents which are not target\n",
    "# .. and check that the target embedding is usually much closer. That'd be a good indicator that embeddings are working as expected\n",
    "# but will not inform us onthe exact performance of the whole retrieval process.\n",
    "def compare_question_embedding_to_target_embedding(model, question_element):\n",
    "    question_embedding = retrieve_embedding_method(model, question_element[\"question\"])\n",
    "    target_id = question_element[\"chunk_id\"] # corresponds to the exact id in ds_corpus\n",
    "    rnd_id = np.random.randint(len(ds_corpus))\n",
    "\n",
    "    target_embedding = retrieve_embedding_method(model, ds_corpus[target_id][\"text_chunk\"])\n",
    "    random_doc_embedding = retrieve_embedding_method(model, ds_corpus[rnd_id][\"text_chunk\"])\n",
    "\n",
    "    return {\n",
    "        \"target_emb_similarity\": cos_sim(question_embedding, target_embedding),\n",
    "        \"random_emb_similarity\": cos_sim(question_embedding, random_doc_embedding)\n",
    "    }\n",
    "\n",
    "ds_qas = ds_qas.map(\n",
    "    lambda element: compare_question_embedding_to_target_embedding(model, element)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e99f751-fd7f-42ad-b3c5-cee5e7994b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ds_qas[\"target_emb_similarity\"], bins=100, alpha=0.5)\n",
    "plt.hist(ds_qas[\"random_emb_similarity\"], bins=100, alpha=0.5)\n",
    "plt.title(\"Embeddings similarity distribution for target chunk vs a random chunk\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c080965f-3d84-4522-a350-abc28299a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the histograms are fairly well separated, we can say the embeddings are working as intended, but that doesn't\n",
    "# mean we'll retrieve the right documents everytime. To check this we'll need to actual run the kNN search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929612c-8d6f-4af9-8076-6c90edb0e869",
   "metadata": {},
   "source": [
    "## B) Exact evaluation with kNN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "336afa2e-c765-4c38-a139-f8a7f38de416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will borrow a few things from the next notebook here that dives into more details on the different indexing methods we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1873b9d-24e0-4d89-a4fc-4a4241ef4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = np.array(ds_corpus[\"embedding\"])\n",
    "\n",
    "# Build index\n",
    "d = len(ds_corpus[0][\"embedding\"])\n",
    "flat_index = faiss.IndexFlatIP(d)\n",
    "flat_index.add(all_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12547ad1-7594-4ce1-a780-960bb92815ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_chunk_retrieved_to_target(model, index, question_element):\n",
    "    question_embedding = retrieve_embedding_method(model, question_element[\"question\"])\n",
    "    target_id = question_element[\"chunk_id\"] # corresponds to the exact id in ds_corpus\n",
    "    \n",
    "    query = question_embedding.numpy()[np.newaxis, :]\n",
    "\n",
    "    distances, retrieved_ids = index.search(query, 1)\n",
    "\n",
    "    return {\n",
    "        \"found_right_match\": target_id == retrieved_ids[0][0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4344af5-38a1-4900-996b-be2d4c1f4983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a576e52b3048539e58d7e28950f680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_qas = ds_qas.map(\n",
    "    lambda element: compare_chunk_retrieved_to_target(model, flat_index, element)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4960eca1-02f0-4f98-ab4d-ec1943425b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We retrieved the right document in 52 of 96 cases\n",
      "Accuracy top result: 0.54\n"
     ]
    }
   ],
   "source": [
    "accuracy_top_result = sum(ds_qas['found_right_match']) / len(ds_qas)\n",
    "print(f\"We retrieved the right document in {sum(ds_qas['found_right_match'])} of {len(ds_qas)} cases\")\n",
    "print(f\"Accuracy top result: {accuracy_top_result:.2f}\")\n",
    "\n",
    "# [Exercise] We are assuming only one context chunk is relevant here and we return only one, which\n",
    "# makes the evaluation metric quite straightforward. How would that change if multiple text chunks\n",
    "# could be considered as relevant? What about if we return multiple? (Look closer to a multi-class\n",
    "# multi-label ML problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc26b2a6-2d20-4c29-acaa-2d99fd561e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Exercise]: what about checking Recall @ top 3 results?\n",
    "# [Exercise]: can we find patterns in the mistakes we're making?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cae6ffde-ebb5-46f8-b305-2afcc0dcb8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-06 18:44:07,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to mps (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "W1106 18:44:07.595360 8280846912 torch/distributed/elastic/multiprocessing/redirects.py:28] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# Save the embedding model/index back for reuse\n",
    "model.save(os.path.join(LOCAL_DATASET_FOLDER, f\"embedding_model_{CORPUS_DATASET_NAME}.model\"))\n",
    "\n",
    "faiss.write_index(flat_index, os.path.join(LOCAL_DATASET_FOLDER, f\"flat_index_{CORPUS_DATASET_NAME}.index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bea3e-670f-4771-8361-5cd7436cc407",
   "metadata": {},
   "source": [
    "# Section 3: Fine tune retriever models to improve embeddings matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d373329-1ba5-4407-a148-b9a4068a8744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd72835e8bd048e3bd4ec4e5ecfa7c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d702c73589469882063ca2ac45c972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will finetune the MiniLM model to better match questions -> text_chunks\n",
    "ds_qas_similar = ds_qas.map(\n",
    "    lambda example: {\n",
    "        \"text_chunk\": ds_corpus[example[\"chunk_id\"]][\"text_chunk\"],  # joins info together,\n",
    "        \"label\": 1.0 # they are supposed to match\n",
    "    }\n",
    ").select_columns([\"text_chunk\", \"question\", \"label\"])\n",
    "\n",
    "ds_qas_dissimilar = ds_qas.map(\n",
    "    lambda example: {\n",
    "        \"text_chunk\": ds_corpus[np.random.randint(len(ds_qas))][\"text_chunk\"],  # joins info together,\n",
    "        \"label\": 0.0 # they are not supposed to match, hoping random gets it right\n",
    "    }\n",
    ").select_columns([\"text_chunk\", \"question\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c61bdf35-93fa-4576-9704-432f911e20ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data = concatenate_datasets(\n",
    "    [ds_qas_similar, ds_qas_dissimilar]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "442fb657-60de-4c7f-b8b5-a8292afd0de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_chunk', 'question', 'label'],\n",
       "    num_rows: 192\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dataset format is (sentenceA, sentenceB, cosinesimilarity)\n",
    "full_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "013f9606-6d00-4b91-8f0a-47606573bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Column 'question' is at index 1, whereas a column with this name is usually expected at index 0. Note that the column order can be important for some losses, e.g. MultipleNegativesRankingLoss will always consider the first column as the anchor and the second as the positive, regardless of the dataset column names. Consider renaming the columns to match the expected order, e.g.:\n",
      "dataset = dataset.select_columns(['question', 'answer'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.008872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.019357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.021867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.020888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.019719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.05413608491420746, metrics={'train_runtime': 8.4541, 'train_samples_per_second': 90.489, 'train_steps_per_second': 5.914, 'total_flos': 0.0, 'train_loss': 0.05413608491420746, 'epoch': 5.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a loss function\n",
    "loss = CosineSimilarityLoss(model)\n",
    "\n",
    "ratio_eval = 0.2\n",
    "num_examples_train = int(full_training_data.num_rows * (1 - ratio_eval))\n",
    "\n",
    "# Specify training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=\"models/test\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Create a trainer & train\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=full_training_data.select(\n",
    "        range(num_examples_train)\n",
    "    ),\n",
    "    eval_dataset=full_training_data.select(\n",
    "        range(num_examples_train, full_training_data.num_rows)\n",
    "    ),\n",
    "    loss=loss,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69231e5-9138-4d84-a4c1-3b3ddc64388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/271938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Exercise] We could recompute embeddings on the corpus and reindex those, or simply \n",
    "# compute embeddings questions with the new model and 'hope' for better results. Feel\n",
    "# free to try both options!\n",
    "ds_corpus_new_embeddings = load_from_disk(\n",
    "    os.path.join(LOCAL_DATASET_FOLDER, CORPUS_DATASET_NAME)\n",
    ")\n",
    "\n",
    "ds_corpus_new_embeddings = ds_corpus_new_embeddings.map(\n",
    "    lambda element: {\"embedding\": retrieve_embedding_method(model, element[\"text_chunk\"])},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1adc7e0a-82a2-424c-a9cb-32f528e433a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors_w_new_embeddings = np.array(ds_corpus_new_embeddings[\"embedding\"])\n",
    "\n",
    "# Build index\n",
    "d = len(ds_corpus_new_embeddings[0][\"embedding\"])\n",
    "flat_index_w_new_embeddings = faiss.IndexFlatIP(d)\n",
    "flat_index_w_new_embeddings.add(all_vectors_w_new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4791d273-155f-41cf-83c9-ffca303665bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun map with updated model\n",
    "ds_qas_new_embeddings = load_from_disk(os.path.join(LOCAL_DATASET_FOLDER, QUESTIONS_DATASET_NAME))\n",
    "ds_qas_new_embeddings = ds_qas_new_embeddings.map(\n",
    "    lambda element: {\"embedding\": retrieve_embedding_method(model, element[\"question\"])},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c277cf88-6084-4786-a5bb-e8256799d076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0b3bed93cc4f7b8fdf2b021960241f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_qas_new_embeddings = ds_qas_new_embeddings.map(\n",
    "    lambda element: compare_chunk_retrieved_to_target(model, flat_index_w_new_embeddings, element)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec2cbf92-2f46-4ab8-bd4b-4c32e5d1d945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We retrieved the right document in 90 of 94 cases\n",
      "Accuracy top result: 0.96\n"
     ]
    }
   ],
   "source": [
    "accuracy_top_result = sum(ds_qas_new_embeddings['found_right_match']) / len(ds_qas_new_embeddings)\n",
    "print(f\"We retrieved the right document in {sum(ds_qas_new_embeddings['found_right_match'])} of {len(ds_qas_new_embeddings)} cases\")\n",
    "print(f\"Accuracy top result: {accuracy_top_result:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae734b7-c138-4794-8086-69582f892f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are doing much much better BUT most of the examples were used to train the fine tuned model...\n",
    "# We should iterate on a bigger dataset and a proper hold out group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
