{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook is part of a series of notebooks that aim to reuse open-source LLM models to perform a binary classification task.\n",
    "\n",
    "Notebooks can be run completely independently from the others and besides dataset_utils.py have no common local dependencies. (As a result,\n",
    "you can expect a little bit of code redundancy between notebooks) \n",
    "\n",
    "**The task is to detect toxic comments out of text comments retrieved from different news websites.**\n",
    "\n",
    "For more information, see dataset_utils.py or search for 'Civil Comments dataset' online.\n",
    "\n",
    "-----\n",
    "This notebook loads models locally via the Hugging Face Transformers package and **performs Few-shots learning classifications**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cache is False\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Mapping, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fe107d66d448b1b331d31f8cc0aa87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6656fb8127e84e49b857a7093428e7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f22e83dd6c4058b401724afceafde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads only a sample of the dataset for quick experiments!\n",
    "comments_dataset = dataset_utils.load_sampled_ds(ds_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our dataset already has 3 splits ready\n",
    "\n",
    "# Our target is the 'is_toxic' binary column\n",
    "# The main feature we'll use is the free text 'text' column\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is mapped to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Pick a model or try add a different one you'd like to experiment with!\n",
    "# for gated repos, you need to login - huggingface-cli login\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # Ok on T4 - https://huggingface.co/Qwen/Qwen2.5-1.5B\n",
    "#model_name = \"microsoft/Phi-3.5-mini-instruct\" # Ok on T4 - https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
    "#model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" # not tested, should be ok on T4 - https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "#model_name = \"meta-llama/Llama-3.2-1B\" # not tested, should be ok on T4 - https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "#model_name = \"facebook/MobileLLM-1B\"  # not tested, should be ok on T4 - https://huggingface.co/facebook/MobileLLM-1B\n",
    "#model_name = \"google/gemma-2-2b-it\"  # not tested, should be ok on T4 - https://huggingface.co/google/gemma-2-2b-it\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" # Ok on A100 - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "#model_name = \"Qwen/Qwen2.5-7B-Instruct\" # Ok on A100 - https://huggingface.co/Qwen/Qwen2.5-7B\n",
    "#model_name = \"meta-llama/Llama-3.2-3B-Instruct\" # ok on A100 # https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"model is mapped to {model.device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left' # See tokenise_batch_and_generate_prompt for explanations!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Few-Shots learning predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff374e8058ae4e80949de86df80893cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b4fc3362b44d99abc0a6db19899e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abdc34c8a15542aba302b984aba8ebfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4eae77189c43aeb7dc0ac806f59bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278c36dd33e841f99f624c5022f81dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b2025aaf545a897d2e79bc9810ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66d68d0680442f891bf9d3198c5d3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fa16df3d8146acbc555a3488a91055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8b514b6cf945c79da7da877f44a2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_comments = comments_dataset.filter(lambda example:example[\"is_toxic\"])[\"train\"][\"text\"]\n",
    "non_toxic_comments = comments_dataset.filter(lambda example:not example[\"is_toxic\"])[\"train\"][\"text\"]\n",
    "\n",
    "def build_few_shots_based_prompt(example):\n",
    "    # Tune this to get more examples in the context, make sure context is wide enough to hold everything\n",
    "    N_EXAMPLES_PER_LABEL = 5\n",
    "\n",
    "    few_shots_examples = \"Here are a few examples of comments and whether there are toxic or not:\\n\"\n",
    "    # [Exercise] How to better sample toxic and non toxic comments here?\n",
    "    for comment in toxic_comments[:N_EXAMPLES_PER_LABEL]:\n",
    "        few_shots_examples += f\"- '{comment}' -> is toxic. \\n\"\n",
    "    for comment in non_toxic_comments[:N_EXAMPLES_PER_LABEL]:\n",
    "        few_shots_examples += f\"- '{comment}' -> is NOT toxic.\\n\"\n",
    "    \n",
    "    prefix = \"Is the following comment in quotes toxic?\"\n",
    "    suffix = \"Please answer by Yes only or No only, nothing else. \\n\"\n",
    "\n",
    "    comment = example[\"text\"]\n",
    "    full_prompt = f\"{few_shots_examples} \\n {prefix} '{comment}'. {suffix}\"\n",
    "\n",
    "    return {\"prompt\": full_prompt}\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(build_few_shots_based_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Here is the first input of the batch to the model after applying chat template:\n",
      "---\n",
      "<|im_start|>system\n",
      "You are a helpful assistant that helps users determine if a comment found on a news platform would be considered toxic. The definition of 'toxic comment' here would be: 'a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective'<|im_end|>\n",
      "<|im_start|>user\n",
      "Here are a few examples of comments and whether there are toxic or not:\n",
      "- 'And I always thought that Eric Trump was the dumbest son.  Sorry, Eric!' -> is toxic. \n",
      "- 'Careful, your misogyny is showing.' -> is toxic. \n",
      "- 'Yeah it does, why? Because all these damn people come here on vacation or whatever reason, and fall in love and friggin move here. The problem with that is the more that have come - the less we remain what everyone loved!\n",
      "I've lived here my entire life and I've watched as we have become less and less \"green & beautiful\" and more \"concrete & cookie cutter\"! The most \"scandalous\" thing we had growing up were those free loving, pot head hippies ... oh how I long for the old Oregon and her worries!' -> is toxic. \n",
      "- 'All this cr#p going on and TWO people get fired?!?!!? That'll fix it!' -> is toxic. \n",
      "- 'Hey Les, among the things I dislike - no, make that despise - are hypocrits, opportunists, bigots and bullies.  In other words, Trump and his supporters.  And no, I'm not in need of \"free stuff.\"  My educational and income levels are probably above yours - putting me well into the Hillary demographic.  Think about it.' -> is toxic. \n",
      "- 'Socialists? What the hell does that have to do with climate science? Good grief.' -> is NOT toxic.\n",
      "- 'I’m disappointed there’s no report from CB on the transportation committee meeting today and the discussion about eminent domain.  It centered on the Bloodbank and the HART CEO’s lack of knowledge that HART filed a lawsuit against the Bloodbank the day before Councilmember Anderson asked about the status of that property (May 16).\n",
      "\n",
      "To sum it up, Murthy nor the board knew that HART was suing the Bloodbank.  It’s troubling that a lawsuit can be started without explicit authority from the CEO and board.\n",
      "\n",
      "The other part of the discussion that was disappointing was the deputy corporation counsel’s suggestion that the committee go into executive session to discuss the issue further.' -> is NOT toxic.\n",
      "- 'Nothing like subsidizing a country that is threatening trade barriers with us.\n",
      "\n",
      "But again we are talking about Kathleen and her cronies.  I do love electric cars, but in a province that still allows gas blowing leaf blowers, classic cars that admit 100 times what normal ICE emit, two cycle outboard marine engines, idiotic two cycle jet skis and snowmobiles....and the list goes on and on. If she wants real easy environmental solutions get rid of these dinosaur beasts. Its easy, politically tough in certain jurisdictions, and quit pandering to the US car companies and wealthy individuals who can afford a 120K automomobile.\n",
      "\n",
      "IF it was an environmental issue all of the above would have been addressed by now.' -> is NOT toxic.\n",
      "- 'The only way to save the rhino is to legalize the trade in horn and allow the farmers to breed the animals and make a profit. Nobody is seriously thinking that legal trade will satisfy the demand or stop the poaching entirely, but farmers will have enough money to jack up security. Rhino numbers will undoubtedly increase, and isn't that the primary consideration? One thing we know: the ban is a disaster. This entire poaching catastrophe has taken place with the BAN INTACT. We can't be entirely sure what will work, but we know without a doubt what is not working: the BAN. Kill the ban or kill the rhino.' -> is NOT toxic.\n",
      "- 'If they paid more than $14 an hour to do the work they would be able to find Americans to work.  Of course all the landscaping companies would have to hire American workers and the cost of landscaping would go up.' -> is NOT toxic.\n",
      " \n",
      " Is the following comment in quotes toxic? 'I hate you, you are dumb'. Please answer by Yes only or No only, nothing else. \n",
      "<|im_end|>\n",
      "\n",
      "---\n",
      "Here is the output of the model after applying chat template:\n",
      "---\n",
      "system\n",
      "You are a helpful assistant that helps users determine if a comment found on a news platform would be considered toxic. The definition of 'toxic comment' here would be: 'a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective'\n",
      "user\n",
      "Here are a few examples of comments and whether there are toxic or not:\n",
      "- 'And I always thought that Eric Trump was the dumbest son.  Sorry, Eric!' -> is toxic. \n",
      "- 'Careful, your misogyny is showing.' -> is toxic. \n",
      "- 'Yeah it does, why? Because all these damn people come here on vacation or whatever reason, and fall in love and friggin move here. The problem with that is the more that have come - the less we remain what everyone loved!\n",
      "I've lived here my entire life and I've watched as we have become less and less \"green & beautiful\" and more \"concrete & cookie cutter\"! The most \"scandalous\" thing we had growing up were those free loving, pot head hippies ... oh how I long for the old Oregon and her worries!' -> is toxic. \n",
      "- 'All this cr#p going on and TWO people get fired?!?!!? That'll fix it!' -> is toxic. \n",
      "- 'Hey Les, among the things I dislike - no, make that despise - are hypocrits, opportunists, bigots and bullies.  In other words, Trump and his supporters.  And no, I'm not in need of \"free stuff.\"  My educational and income levels are probably above yours - putting me well into the Hillary demographic.  Think about it.' -> is toxic. \n",
      "- 'Socialists? What the hell does that have to do with climate science? Good grief.' -> is NOT toxic.\n",
      "- 'I’m disappointed there’s no report from CB on the transportation committee meeting today and the discussion about eminent domain.  It centered on the Bloodbank and the HART CEO’s lack of knowledge that HART filed a lawsuit against the Bloodbank the day before Councilmember Anderson asked about the status of that property (May 16).\n",
      "\n",
      "To sum it up, Murthy nor the board knew that HART was suing the Bloodbank.  It’s troubling that a lawsuit can be started without explicit authority from the CEO and board.\n",
      "\n",
      "The other part of the discussion that was disappointing was the deputy corporation counsel’s suggestion that the committee go into executive session to discuss the issue further.' -> is NOT toxic.\n",
      "- 'Nothing like subsidizing a country that is threatening trade barriers with us.\n",
      "\n",
      "But again we are talking about Kathleen and her cronies.  I do love electric cars, but in a province that still allows gas blowing leaf blowers, classic cars that admit 100 times what normal ICE emit, two cycle outboard marine engines, idiotic two cycle jet skis and snowmobiles....and the list goes on and on. If she wants real easy environmental solutions get rid of these dinosaur beasts. Its easy, politically tough in certain jurisdictions, and quit pandering to the US car companies and wealthy individuals who can afford a 120K automomobile.\n",
      "\n",
      "IF it was an environmental issue all of the above would have been addressed by now.' -> is NOT toxic.\n",
      "- 'The only way to save the rhino is to legalize the trade in horn and allow the farmers to breed the animals and make a profit. Nobody is seriously thinking that legal trade will satisfy the demand or stop the poaching entirely, but farmers will have enough money to jack up security. Rhino numbers will undoubtedly increase, and isn't that the primary consideration? One thing we know: the ban is a disaster. This entire poaching catastrophe has taken place with the BAN INTACT. We can't be entirely sure what will work, but we know without a doubt what is not working: the BAN. Kill the ban or kill the rhino.' -> is NOT toxic.\n",
      "- 'If they paid more than $14 an hour to do the work they would be able to find Americans to work.  Of course all the landscaping companies would have to hire American workers and the cost of landscaping would go up.' -> is NOT toxic.\n",
      " \n",
      " Is the following comment in quotes toxic? 'I hate you, you are dumb'. Please answer by Yes only or No only, nothing else. \n",
      "\n",
      "assistant\n",
      "Yes\n",
      "---\n",
      "Here is the decoded answer, after removing the inputs\n",
      "---\n",
      "Yes\n",
      "---\n",
      "Here is the final binary output:\n",
      "---\n",
      "is_toxic = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': [True, False]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenise in batches to optimise for speed!\n",
    "def tokenise_batch_and_generate_answer(\n",
    "    example_batch: Mapping[str, Iterable[Any]],\n",
    "    print_intermediate_outputs: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    This method is a simple implementation of zero-shot classification on a input batch of examples.\n",
    "    It Tokenises the batch and apply the model on it. It then parses the results to return a binary output.\n",
    "\n",
    "    Optionally, `print_intermediate_outputs` makes the intermediate outputs visible at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    messages_batch = [\n",
    "      [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps users determine if a comment found on a news platform would be considered toxic. The definition of 'toxic comment' here would be: 'a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "      for prompt in example_batch[\"prompt\"]\n",
    "    ]\n",
    "\n",
    "    # !!! We are not only doing text autocompletion here but we need to make sure\n",
    "    # the prompts we are creating correspond to the 'template' the model\n",
    "    # learnt (different chat templates for different models)\n",
    "    inputs_batch = tokenizer.apply_chat_template(\n",
    "        messages_batch,\n",
    "        add_generation_prompt=True, # indicate we are expecting an answer from the model\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, # inputs that are too long will be truncated, we should check the context size of the model\n",
    "        padding=True, # not all inputs have the same length,\n",
    "        # make sure tokeniser has padding_side='left' because we are padding all inputs to the same size\n",
    "    )\n",
    "    \n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the first input of the batch to the model after applying chat template:\")\n",
    "        print(\"---\")\n",
    "        print(tokenizer.apply_chat_template(messages_batch[0], tokenize=False))\n",
    "\n",
    "    # Mapping input to GPU for faster processing\n",
    "    inputs_mapped_to_device_batch = {k: v.to(model.device) for k, v in inputs_batch.items()}\n",
    "\n",
    "    # [Exercise] If you added a 'long' initial prompt, you are recomputing the same initial attention\n",
    "    # values multiple times, how can you optimise for processing speed?\n",
    "    generated_ids_batch = model.generate(\n",
    "        **inputs_mapped_to_device_batch,\n",
    "        do_sample=False, # no need to be creative here\n",
    "        max_new_tokens=5, # We are expecting even less (1 word and perhaps some punctuation)\n",
    "    )\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the output of the model after applying chat template:\")\n",
    "        print(\"---\")\n",
    "        print(tokenizer.batch_decode(generated_ids_batch, skip_special_tokens=True)[0])\n",
    "\n",
    "    # Generated_ids also contain input_ids that we need to filter out\n",
    "    generated_ids_batch = generated_ids_batch[:, inputs_batch[\"input_ids\"].shape[1]:]\n",
    "\n",
    "    decoded_answers_batch = tokenizer.batch_decode(generated_ids_batch, skip_special_tokens=True)\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the decoded answer, after removing the inputs\")\n",
    "        print(\"---\")\n",
    "        print(decoded_answers_batch[0])\n",
    "\n",
    "    # !!! we are returning False if the model doesn't say Yes or No back\n",
    "    # [Exercise] How can we constrain the model to return only certain tokens?\n",
    "    def get_binary_output(answer: str)-> bool:\n",
    "        return True if 'yes' in answer.lower() else False\n",
    "\n",
    "    binary_output_batch = [get_binary_output(answer) for answer in decoded_answers_batch]\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the final binary output:\")\n",
    "        print(\"---\")\n",
    "        print(f\"is_toxic = {binary_output_batch[0]}\")\n",
    "\n",
    "\n",
    "    return {\"prediction\": binary_output_batch}\n",
    "\n",
    "# Try out an example batch\n",
    "tokenise_batch_and_generate_answer(\n",
    "    {\"prompt\":[build_few_shots_based_prompt({\"text\": \"I hate you, you are dumb\"})[\"prompt\"], build_few_shots_based_prompt({\"text\": \"I love you\"})[\"prompt\"]]},\n",
    "    print_intermediate_outputs=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f5270e36fb45f1a404d04dc19e75f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Map method to dataset\n",
    "# Tune parameters to adjust for memory/storage space reqs of your env\n",
    "# [Exercise] Is your GPU memory well utilised? if not, how to tune parameters towards better GPU utilisation?\n",
    "comments_dataset = comments_dataset.map(\n",
    "    tokenise_batch_and_generate_answer,\n",
    "    keep_in_memory=True,\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=16,\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f5270e36fb45f1a404d04dc19e75f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Map method to dataset\n",
    "# Tune parameters to adjust for memory/storage space reqs of your env\n",
    "# [Exercise] Is your GPU memory well utilised? if not, how to tune parameters towards better GPU utilisation?\n",
    "comments_dataset = comments_dataset.map(\n",
    "    tokenise_batch_and_generate_answer,\n",
    "    keep_in_memory=True,\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=16,\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6488772ae8814037b1fdbca28c2417f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69692c53fe7f4dba8b6cde745a2849ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb61824fb536400aa0feb70d47042b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2367bbe3ac48b493cd6e0ae2491036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4,\n",
       " 'f1': 0.25,\n",
       " 'precision': 0.14492753623188406,\n",
       " 'recall': 0.9090909090909091}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not expecting awesome accuracy, but at least it enables to single out a few examples,\n",
    "# so it is in a way more useful than a dummy baseline\n",
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"validation\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"validation\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.34,\n",
       " 'f1': 0.15384615384615385,\n",
       " 'precision': 0.08333333333333333,\n",
       " 'recall': 1.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you're happy with your tuning, run the evaluation on the test set and report your results on the sheet!\n",
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"test\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"test\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
