{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook is part of a series of notebooks that aim to reuse open-source LLM models to perform a binary classification task.\n",
    "\n",
    "Notebooks can be run completely independently from the others and besides dataset_utils.py have no common local dependencies. (As a result,\n",
    "you can expect a little bit of code redundancy between notebooks) \n",
    "\n",
    "**The task is to detect toxic comments out of text comments retrieved from different news websites.**\n",
    "\n",
    "For more information, see dataset_utils.py or search for 'Civil Comments dataset' online.\n",
    "\n",
    "-----\n",
    "This notebook loads models locally via the Hugging Face Transformers package and **performs Zero-shot classifications**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cache is False\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Mapping, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6671a542a84e0aa7b62bfa9acd7726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39185d72f0454967b4e92afa3df87210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/194M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cd41b2328340c38522617a57220f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/187M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f970bc266c854e7689e74d45b7176832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723e10aa376d43ca86d82c38d252341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "333ba2f5bec54316afb645efe598bb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1804874 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372166e12c0542f9927bf714f83532cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6794974fa5234c88904a5a740cb49ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/97320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1104da1564e54761a2885138b2a6b2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159cdab8c5fe4b2c98f8c1b5c328ad77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef26e3498504e8fa780e8d62d364752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads only a sample of the dataset for quick experiments!\n",
    "comments_dataset = dataset_utils.load_sampled_ds(ds_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our dataset already has 3 splits ready\n",
    "\n",
    "# Our target is the 'is_toxic' binary column\n",
    "# The main feature we'll use is the free text 'text' column\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fde55e9b8041a5aa10ecb289c7b920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507cc65b830f4cb5b0fdd08917167005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb02c9444694e1da183c003564e9853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is mapped to cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d7bbf12f74d94bdab58b10aae4bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef5dc7e95e648fda355eb5c4ac6fc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118b8a5d33974ecb9e734e5ccd7b3f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc955dfe132540db9eada4aa4c153fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick a model or try add a different one you'd like to experiment with!\n",
    "# for gated repos, you need to login - huggingface-cli login\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # Ok on T4 - https://huggingface.co/Qwen/Qwen2.5-1.5B\n",
    "#model_name = \"microsoft/Phi-3.5-mini-instruct\" # Ok on T4 - https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
    "#model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" # not tested, should be ok on T4 - https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "#model_name = \"meta-llama/Llama-3.2-1B\" # not tested, should be ok on T4 - https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "#model_name = \"facebook/MobileLLM-1B\"  # not tested, should be ok on T4 - https://huggingface.co/facebook/MobileLLM-1B\n",
    "#model_name = \"google/gemma-2-2b-it\"  # not tested, should be ok on T4 - https://huggingface.co/google/gemma-2-2b-it\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" # Ok on A100 - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "#model_name = \"Qwen/Qwen2.5-7B-Instruct\" # Ok on A100 - https://huggingface.co/Qwen/Qwen2.5-7B\n",
    "#model_name = \"meta-llama/Llama-3.2-3B-Instruct\" # ok on A100 # https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"model is mapped to {model.device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left' # See tokenise_batch_and_generate_prompt for explanations!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zero-Shot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c0c0577e024277933fb26a5b897281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189f3e8e95e14363a4518f5b7654ffaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a18a5bf66047ac9fa82877d52d5a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format comment 'text' field into a _user_ prompt \n",
    "def build_basic_prompt(example: Mapping[str, Any]):\n",
    "    # [Exercise] How to improve the prompt being used here?\n",
    "    # [Exercise] Let's say we find the model is biased towards always answering 'No', how would we tune the prompt to steer it the other direction?\n",
    "    prefix = \"Is the following comment in quotes toxic?\"\n",
    "    suffix = \"Please answer by Yes only or No only, nothing else.\"\n",
    "    comment = example[\"text\"]\n",
    "    full_prompt = f\"{prefix} '{comment}'. {suffix}\"\n",
    "\n",
    "    return {\"prompt\": full_prompt}\n",
    "\n",
    "# Add to dataset\n",
    "comments_dataset = comments_dataset.map(build_basic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Here is the first input of the batch to the model after applying chat template:\n",
      "---\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is the following comment in quotes toxic? 'I hate you, you are dumb'. Please answer by Yes only or No only, nothing else.<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Here is the output of the model after applying chat template:\n",
      "---\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Is the following comment in quotes toxic? 'I hate you, you are dumb'. Please answer by Yes only or No only, nothing else.\n",
      "assistant\n",
      "Yes\n",
      "---\n",
      "Here is the decoded answer, after removing the inputs\n",
      "---\n",
      "Yes\n",
      "---\n",
      "Here is the final binary output:\n",
      "---\n",
      "is_toxic = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': [True, False]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenise in batches to optimise for speed!\n",
    "def tokenise_batch_and_generate_answer(example_batch: Mapping[str, Iterable[Any]], print_intermediate_outputs: bool = False):\n",
    "    \"\"\"\n",
    "    This method is a simple implementation of zero-shot classification on a input batch of examples.\n",
    "    It Tokenises the batch and apply the model on it. It then parses the results to return a binary output.\n",
    "\n",
    "    Optionally, `print_intermediate_outputs` makes the intermediate outputs visible at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    messages_batch = [\n",
    "      # [Exercise] Some models can expect in their template a system prompt to tune their behaviour, we can try with and without and observe\n",
    "      [\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "      for prompt in example_batch[\"prompt\"]\n",
    "    ]\n",
    "\n",
    "    # !!! We are not only doing text autocompletion here but we need to make sure\n",
    "    # the prompts we are creating correspond to the 'template' the model\n",
    "    # learnt (different chat templates for different models)\n",
    "    inputs_batch = tokenizer.apply_chat_template(\n",
    "        messages_batch,\n",
    "        add_generation_prompt=True, # indicate we are expecting an answer from the model\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, # inputs that are too long will be truncated, we should check the context size of the model\n",
    "        padding=True, # not all inputs have the same length,\n",
    "        # make sure tokeniser has padding_side='left' because we are padding all inputs to the same size\n",
    "    )\n",
    "    \n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the first input of the batch to the model after applying chat template:\")\n",
    "        print(\"---\")\n",
    "        print(tokenizer.apply_chat_template(messages_batch[0], tokenize=False))\n",
    "\n",
    "    # Mapping input to GPU for faster processing\n",
    "    inputs_mapped_to_device_batch = {k: v.to(model.device) for k, v in inputs_batch.items()}\n",
    "\n",
    "    # [Exercise] If you added a 'long' initial prompt, you are recomputing the same initial attention\n",
    "    # values multiple times, how can you optimise for processing speed?\n",
    "    generated_ids_batch = model.generate(\n",
    "        **inputs_mapped_to_device_batch,\n",
    "        do_sample=False, # no need to be creative here\n",
    "        max_new_tokens=5 # We are expecting even less (1 word and perhaps some punctuation)\n",
    "    )\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the output of the model after applying chat template:\")\n",
    "        print(\"---\")\n",
    "        print(tokenizer.batch_decode(generated_ids_batch, skip_special_tokens=True)[0])\n",
    "\n",
    "    # Generated_ids also contain input_ids that we need to filter out\n",
    "    generated_ids_batch = generated_ids_batch[:, inputs_batch[\"input_ids\"].shape[1]:]\n",
    "\n",
    "    decoded_answers_batch = tokenizer.batch_decode(generated_ids_batch, skip_special_tokens=True)\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the decoded answer, after removing the inputs\")\n",
    "        print(\"---\")\n",
    "        print(decoded_answers_batch[0])\n",
    "\n",
    "    # !!! we are returning False if the model doesn't say Yes or No back\n",
    "    # [Exercise] How can we constrain the model to return only certain tokens?\n",
    "    def get_binary_output(answer: str)-> bool:\n",
    "        return True if 'yes' in answer.lower() else False\n",
    "\n",
    "    binary_output_batch = [get_binary_output(answer) for answer in decoded_answers_batch]\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the final binary output:\")\n",
    "        print(\"---\")\n",
    "        print(f\"is_toxic = {binary_output_batch[0]}\")\n",
    "\n",
    "    return {\"prediction\": binary_output_batch}\n",
    "\n",
    "# Try out an example batch\n",
    "tokenise_batch_and_generate_answer(\n",
    "    {\"prompt\":[build_basic_prompt({\"text\": \"I hate you, you are dumb\"})[\"prompt\"], build_basic_prompt({\"text\": \"I love you\"})[\"prompt\"]]},\n",
    "    print_intermediate_outputs=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96b275004fc4afbb64ac04b5937d65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5055bd521247b3ac04b27832630cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6654590b1841ffa96932b4fcbb9c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map method to dataset\n",
    "# Tune parameters to adjust for memory/storage space reqs of your env\n",
    "# [Exercise] Is your GPU memory well utilised? if not, how to tune parameters towards better GPU utilisation?\n",
    "comments_dataset = comments_dataset.map(\n",
    "    tokenise_batch_and_generate_answer,\n",
    "    keep_in_memory=True,\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=16,\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31f720f3161436b8f9eb25a2befd339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84d6f1136054437b48c876ab2cc91a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db7389741fc4ddcb4969983f8439bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb7875995fb4385abf062a2878dbd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.39,\n",
       " 'f1': 0.24691358024691357,\n",
       " 'precision': 0.14285714285714285,\n",
       " 'recall': 0.9090909090909091}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not expecting awesome accuracy, but at least it enables to single out a few examples,\n",
    "# so it is in a way more useful than a dummy baseline\n",
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"validation\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"validation\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.325,\n",
       " 'f1': 0.15094339622641506,\n",
       " 'precision': 0.08163265306122448,\n",
       " 'recall': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you're happy with your tuning, run the evaluation on the test set and report your results on the sheet!\n",
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"test\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"test\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
