{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook is part of a series of notebooks that aim to reuse open-source LLM models to perform a binary classification task.\n",
    "\n",
    "Notebooks can be run completely independently from the others and besides dataset_utils.py have no common local dependencies. (As a result,\n",
    "you can expect a little bit of code redundancy between notebooks) \n",
    "\n",
    "**The task is to detect toxic comments out of text comments retrieved from different news websites.**\n",
    "\n",
    "For more information, see dataset_utils.py or search for 'Civil Comments dataset' online.\n",
    "\n",
    "-----\n",
    "This notebook loads models locally via the Hugging Face Transformers package and **performs Zero-shot classifications**.\n",
    "\n",
    "**BUT**, compared to the previous notebook running local zero-shot classifications, this one captures the output logits \n",
    "for the token of interest ('Yes', 'No') and uses the training set to find an optimised threshold that 'calibrates' better\n",
    "what the model thinks with our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cache is False\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Mapping, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a82c1f08d649d1a7358dbd3c012069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb724fd482941ff81cc6a0bcc596e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbc343c9a49416c9c87e2db5e50a2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads only a sample of the dataset for quick experiments!\n",
    "comments_dataset = dataset_utils.load_sampled_ds(ds_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our dataset already has 3 splits ready\n",
    "\n",
    "# Our target is the 'is_toxic' binary column\n",
    "# The main feature we'll use is the free text 'text' column\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is mapped to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Pick a model or try add a different one you'd like to experiment with!\n",
    "# for gated repos, you need to login - huggingface-cli login\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # Ok on T4 - https://huggingface.co/Qwen/Qwen2.5-1.5B\n",
    "#model_name = \"microsoft/Phi-3.5-mini-instruct\" # Ok on T4 - https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
    "#model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\" # not tested, should be ok on T4 - https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\n",
    "#model_name = \"meta-llama/Llama-3.2-1B\" # not tested, should be ok on T4 - https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "#model_name = \"facebook/MobileLLM-1B\"  # not tested, should be ok on T4 - https://huggingface.co/facebook/MobileLLM-1B\n",
    "#model_name = \"google/gemma-2-2b-it\"  # not tested, should be ok on T4 - https://huggingface.co/google/gemma-2-2b-it\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" # Ok on A100 - https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n",
    "#model_name = \"Qwen/Qwen2.5-7B-Instruct\" # Ok on A100 - https://huggingface.co/Qwen/Qwen2.5-7B\n",
    "#model_name = \"meta-llama/Llama-3.2-3B-Instruct\" # ok on A100 # https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"model is mapped to {model.device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side='left' # See tokenise_batch_and_generate_prompt for explanations!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Zero-Shot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd9b7867e974d7796e7044337d2b50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca0515c8d5b4162b844d0a85a8ab42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7522b3c9429242418af6ea4f63efb2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format comment 'text' field into a _user_ prompt \n",
    "def build_basic_prompt(example: Mapping[str, Any]):\n",
    "    # [Exercise] How to improve the prompt being used here? (the definition of 'toxicity' in this task could help for instance?)\n",
    "    # [Exercise] Let's say we find the model is biased towards always answering 'No', how would we tune the prompt to steer it the other direction?\n",
    "    prefix = \"Is the following comment in quotes toxic?\"\n",
    "    suffix = \"Please answer by Yes only or No only, nothing else.\"\n",
    "    comment = example[\"text\"]\n",
    "    full_prompt = f\"{prefix} '{comment}'. {suffix}\"\n",
    "\n",
    "    return {\"prompt\": full_prompt}\n",
    "\n",
    "# Add to dataset\n",
    "comments_dataset = comments_dataset.map(build_basic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Here is the first input of the batch to the model after applying chat template:\n",
      "---\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Is the following comment in quotes toxic? 'I hate you, you are dumb'. Please answer by Yes only or No only, nothing else.<|im_end|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Here is the output of the model after applying chat template:\n",
      "---\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Is the following comment in quotes toxic? 'I hate you, you are dumb'. Please answer by Yes only or No only, nothing else.\n",
      "assistant\n",
      "Yes\n",
      "---\n",
      "With more details, the model thinks the probability that this is Yes (over No) is:\n",
      "99.94%\n",
      "---\n",
      "Here is the decoded answer, after removing the inputs\n",
      "---\n",
      "Yes\n",
      "---\n",
      "Here is the final binary output\n",
      "---\n",
      "is_toxic = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': [True, False],\n",
       " 'yes_prob': array([0.99937373, 0.00669285], dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# /!\\ This method has been slightly modified to also output the model 'confidence' in the token being 'Yes' or 'No'\n",
    "def tokenise_batch_and_generate_answer(example_batch: Mapping[str, Iterable[Any]], print_intermediate_outputs: bool = False):\n",
    "    \"\"\"\n",
    "    This method is a simple implementation of zero-shot classification on a input batch of examples.\n",
    "    It Tokenises the batch and apply the model on it. It then parses the results to return a binary output.\n",
    "    [NEW] Additionally, it inspects the exact output logits for two tokens of interest {'Yes', 'No'} to\n",
    "    run optimisations later.\n",
    "\n",
    "    Optionally, `print_intermediate_outputs` makes the intermediate outputs visible at runtime.\n",
    "    \"\"\"\n",
    "\n",
    "    yes_token_id = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "    no_token_id = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "    \n",
    "    messages_batch = [\n",
    "      # [Exercise] Some models can expect in their template a system prompt to tune their behaviour, we can try with and without and observe\n",
    "      [\n",
    "        # {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "      for prompt in example_batch[\"prompt\"]\n",
    "    ]\n",
    "\n",
    "    # !!! We are not only doing text autocompletion here but we need to make sure\n",
    "    # the prompts we are creating correspond to the 'template' the model\n",
    "    # learnt (different chat templates for different models)\n",
    "    inputs_batch = tokenizer.apply_chat_template(\n",
    "        messages_batch,\n",
    "        add_generation_prompt=True, # indicate we are expecting an answer from the model\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True, # inputs that are too long will be truncated, we should check the context size of the model\n",
    "        padding=True, # not all inputs have the same length,\n",
    "        # make sure tokeniser has padding_side='left' because we are padding all inputs to the same size\n",
    "    )\n",
    "    \n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the first input of the batch to the model after applying chat template:\")\n",
    "        print(\"---\")\n",
    "        print(tokenizer.apply_chat_template(messages_batch[0], tokenize=False))\n",
    "\n",
    "    # Mapping input to GPU for faster processing\n",
    "    inputs_mapped_to_device_batch = {k: v.to(model.device) for k, v in inputs_batch.items()}\n",
    "\n",
    "    # [Exercise] This is not optimised as we are re-computing the attention output for the initial prompt\n",
    "    # for every batch, even though the initial prompt is always the same! how can we used cached outputs?\n",
    "    # https://huggingface.co/docs/transformers/main/kv_cache#best-practices-for-generation-with-cache\n",
    "    generated_ids_batch_dict = model.generate(\n",
    "        **inputs_mapped_to_device_batch,\n",
    "        do_sample=False, # no need to be creative here\n",
    "        max_new_tokens=5, # We are expecting even less (1 word and perhaps some punctuation),\n",
    "        output_logits=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    # Compute actual model probability of 'Yes'\n",
    "    # Only look at first generated token\n",
    "    yes_logits = generated_ids_batch_dict.logits[0][:, yes_token_id]\n",
    "    no_logits = generated_ids_batch_dict.logits[0][:, no_token_id]\n",
    "    yes_no_logits = torch.cat((yes_logits.unsqueeze(1), no_logits.unsqueeze(1)), 1)\n",
    "    yes_no_probs = torch.nn.Softmax(dim=1)(yes_no_logits)\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the output of the model after applying chat template:\")\n",
    "        print(\"---\")\n",
    "        print(tokenizer.batch_decode(generated_ids_batch_dict.sequences, skip_special_tokens=True)[0])\n",
    "        print(\"---\")\n",
    "        print(\"With more details, the model thinks the probability that this is Yes (over No) is:\")\n",
    "        print(f\"{100*yes_no_probs[0][0]:.2f}%\")\n",
    "\n",
    "    # Generated_ids also contain input_ids that we need to filter out\n",
    "    generated_ids_batch = generated_ids_batch_dict.sequences[:, inputs_batch[\"input_ids\"].shape[1]:]\n",
    "\n",
    "    decoded_answers_batch = tokenizer.batch_decode(generated_ids_batch, skip_special_tokens=True)\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the decoded answer, after removing the inputs\")\n",
    "        print(\"---\")\n",
    "        print(decoded_answers_batch[0])\n",
    "\n",
    "    # !!! we are returning False if the model doesn't say Yes or No back\n",
    "    # [Exercise] How could we constrain the model to only return either 'Yes [eos]' or 'No [eos]'?\n",
    "    def get_binary_output(answer: str)-> bool:\n",
    "        return True if 'yes' in answer.lower() else False\n",
    "\n",
    "    binary_output_batch = [get_binary_output(answer) for answer in decoded_answers_batch]\n",
    "\n",
    "    if print_intermediate_outputs:\n",
    "        print(\"---\")\n",
    "        print(\"Here is the final binary output\")\n",
    "        print(\"---\")\n",
    "        print(f\"is_toxic = {binary_output_batch[0]}\")\n",
    "\n",
    "    return {\"prediction\": binary_output_batch, \"yes_prob\": yes_no_probs[:, 0].cpu().numpy()}\n",
    "\n",
    "# Explore an example batch\n",
    "tokenise_batch_and_generate_answer(\n",
    "    {\"prompt\":[build_basic_prompt({\"text\": \"I hate you, you are dumb\"})[\"prompt\"], build_basic_prompt({\"text\": \"I love yoy\"})[\"prompt\"]]},\n",
    "    print_intermediate_outputs=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fad52e06084b848817d706f87fbf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9854a346a3e84a47b5c983318bb7c329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4116a02bae504a9d9d70fa994789aacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Noticed the RAM can blow up quite quickly with this, if unsucessful\n",
    "# pick a dummy for loop approach\n",
    "comments_dataset = comments_dataset.map(\n",
    "    tokenise_batch_and_generate_answer,\n",
    "    keep_in_memory=True,\n",
    "    load_from_cache_file=False,\n",
    "    writer_batch_size=16,\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a good 'yes_prob' threshold candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadc32843ffa487695ef17f8689d757a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229a359a5b4b4e0e8b85548bc326cd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646cfbd29558485e8cd5c3fdb2c73501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad86e98f2c124e4db0b0bc46387b81b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b02c6f05aab4f9c9f7f8254daeac3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022300b42ea4a4cba7a9573b4e26835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the train set to figure out a proper threshold to use!\n",
    "toxic_yes_probs = comments_dataset.filter(lambda example:example[\"is_toxic\"])[\"train\"][\"yes_prob\"]\n",
    "non_toxic_yes_probs = comments_dataset.filter(lambda example:not example[\"is_toxic\"])[\"train\"][\"yes_prob\"]\n",
    "\n",
    "plt.hist(toxic_yes_probs, bins=100, alpha=0.5)\n",
    "plt.hist(non_toxic_yes_probs, bins=100, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232deee447aa4298972c6b958e613387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4c23c2f9284df59d79c1ef7491f10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3106ff92e2d4bfd8c16fc526227c885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(0.9997387528419495,\n",
       "  'Apparently Robert is delusional and would like to pander and hold the hand of a communist dictator who threatens the western world with nuclear holocaust almost daily. North Koreans leaders is a complete nut job. There is no negotiating with a person like that and America damn sure does not need to politely ask how we could help the little fat man.\\n\\nTrump showed true leadership on this issue. Kim knew (because of Trump) if he attacked us or anyone else it would be the last thing he ever did. Literally the last thing. He has wisely backed down. They do this bs with every new administration to see what they can get away with for the time being. He is a child and Trump spanked him.'),\n",
       " (0.9996646642684937,\n",
       "  'LOCK HER UP. She preys on the most needy people in our society. What a horrible human being.'),\n",
       " (0.9996646642684937,\n",
       "  'Islam still holds grudges going back 1000 years.  The Crusades were  actually an attempt to get back for Europe the Christian lands of Turkey, Syria, Lebanon, and most importantly Jerusalem  stolen by Muslims in bloody thirsty conquest.  Islam was determined to exterminate  Christianity, and still is.'),\n",
       " (0.9996646642684937,\n",
       "  \"Don't send it to Africa, they eat albinos there.\\n\\nPardon me, that's only albino humans that they kill and eat.\\n\\nYou think otherwise? Look it up.\"),\n",
       " (0.9995121955871582,\n",
       "  \"Sic em' Rand! McConnell, et al, make us SICK! Traitors to the nation and the people! WE STAND WITH RAND!\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate a bit the top comments the model thought is toxic but is not\n",
    "sorted(zip(toxic_yes_probs, comments_dataset.filter(lambda example:example[\"is_toxic\"])[\"train\"][\"text\"]), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comments are a bit aggressive indeed, but nevertheless not toxic according to the dataset.\n",
    "# The concept of 'Toxicity' can be a bit subtle, and we've not really taught our model to understand it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be tuned to optimise F1 score on train set, or just eyeballed..\n",
    "YES_PROB_THRESHOLD = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.73,\n",
       " 'f1': 0.28947368421052627,\n",
       " 'precision': 0.2037037037037037,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"validation\"][\"is_toxic\"],\n",
    "    predictions=[True if prob>0.99 else False for prob in comments_dataset[\"validation\"][\"yes_prob\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.325,\n",
       " 'f1': 0.15094339622641506,\n",
       " 'precision': 0.08163265306122448,\n",
       " 'recall': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you're happy with your tuning, run the evaluation on the test set and report your results on the sheet!\n",
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"test\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"test\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
