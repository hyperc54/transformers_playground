{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook is part of a series of notebooks that aim to reuse open-source LLM models to perform a binary classification task.\n",
    "\n",
    "Notebooks can be run completely independently from the others and besides dataset_utils.py have no common local dependencies. (As a result,\n",
    "you can expect a little bit of code redundancy between notebooks) \n",
    "\n",
    "**The task is to detect toxic comments out of text comments retrieved from different news websites.**\n",
    "\n",
    "For more information, see dataset_utils.py or search for 'Civil Comments dataset' online.\n",
    "\n",
    "-----\n",
    "This notebook **performs Few-shots learning classifications** via the remote OpenAI API.\n",
    "Using external services enables us to use models that don't fit our env (mostly, GPU constraints).. but can be pricy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets cache is False\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from utils import dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Requires an token registered in env variable OPENAI_API_KEY\n",
    "# or pass api_key keyword argument below\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10e2e955313453ead5519f12ea256a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5f23a5f6b74f38a7a13930b35f5a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f989bb8b2a8f4be682cf1559f169be84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments_dataset = dataset_utils.load_sampled_ds(ds_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit', 'is_toxic'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our dataset already has 3 splits ready\n",
    "\n",
    "# Our target is the 'is_toxic' binary column\n",
    "# The main feature we'll use is the free text 'text' column\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Few-shots learning prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc5226498e54302b725cbf7a22c261b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13a305ce0e4afaa9988cf2c47d2c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6913697f4d4279b9483c5014b432d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bb192a79b64639b398e6e250d37f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de08f8728d64c8db3223b1add4f9935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa468fe5d3a4f2b9e8d6418700f0577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful expert that decides whether a comment is toxic or not for moderation purposes on a social media platform.The definition of 'toxic comment' here would be: 'a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective'\n",
      "Please always answer by saying Yes only or No only, nothing else. \n",
      "Here is a bullet point list of a few examples with their respective labels:\n",
      " - 'And I always thought that Eric Trump was the dumbest son.  Sorry, Eric!' -> is toxic. \n",
      "- 'Careful, your misogyny is showing.' -> is toxic. \n",
      "- 'Yeah it does, why? Because all these damn people come here on vacation or whatever reason, and fall in love and friggin move here. The problem with that is the more that have come - the less we remain what everyone loved!\n",
      "I've lived here my entire life and I've watched as we have become less and less \"green & beautiful\" and more \"concrete & cookie cutter\"! The most \"scandalous\" thing we had growing up were those free loving, pot head hippies ... oh how I long for the old Oregon and her worries!' -> is toxic. \n",
      "- 'All this cr#p going on and TWO people get fired?!?!!? That'll fix it!' -> is toxic. \n",
      "- 'Hey Les, among the things I dislike - no, make that despise - are hypocrits, opportunists, bigots and bullies.  In other words, Trump and his supporters.  And no, I'm not in need of \"free stuff.\"  My educational and income levels are probably above yours - putting me well into the Hillary demographic.  Think about it.' -> is toxic. \n",
      "- 'Socialists? What the hell does that have to do with climate science? Good grief.' -> is NOT toxic.\n",
      "- 'I’m disappointed there’s no report from CB on the transportation committee meeting today and the discussion about eminent domain.  It centered on the Bloodbank and the HART CEO’s lack of knowledge that HART filed a lawsuit against the Bloodbank the day before Councilmember Anderson asked about the status of that property (May 16).\n",
      "\n",
      "To sum it up, Murthy nor the board knew that HART was suing the Bloodbank.  It’s troubling that a lawsuit can be started without explicit authority from the CEO and board.\n",
      "\n",
      "The other part of the discussion that was disappointing was the deputy corporation counsel’s suggestion that the committee go into executive session to discuss the issue further.' -> is NOT toxic.\n",
      "- 'Nothing like subsidizing a country that is threatening trade barriers with us.\n",
      "\n",
      "But again we are talking about Kathleen and her cronies.  I do love electric cars, but in a province that still allows gas blowing leaf blowers, classic cars that admit 100 times what normal ICE emit, two cycle outboard marine engines, idiotic two cycle jet skis and snowmobiles....and the list goes on and on. If she wants real easy environmental solutions get rid of these dinosaur beasts. Its easy, politically tough in certain jurisdictions, and quit pandering to the US car companies and wealthy individuals who can afford a 120K automomobile.\n",
      "\n",
      "IF it was an environmental issue all of the above would have been addressed by now.' -> is NOT toxic.\n",
      "- 'The only way to save the rhino is to legalize the trade in horn and allow the farmers to breed the animals and make a profit. Nobody is seriously thinking that legal trade will satisfy the demand or stop the poaching entirely, but farmers will have enough money to jack up security. Rhino numbers will undoubtedly increase, and isn't that the primary consideration? One thing we know: the ban is a disaster. This entire poaching catastrophe has taken place with the BAN INTACT. We can't be entirely sure what will work, but we know without a doubt what is not working: the BAN. Kill the ban or kill the rhino.' -> is NOT toxic.\n",
      "- 'If they paid more than $14 an hour to do the work they would be able to find Americans to work.  Of course all the landscaping companies would have to hire American workers and the cost of landscaping would go up.' -> is NOT toxic.\n",
      " \n",
      "\n",
      "  That's it, please do good work!\n"
     ]
    }
   ],
   "source": [
    "toxic_comments = comments_dataset.filter(lambda example:example[\"is_toxic\"])[\"train\"][\"text\"]\n",
    "non_toxic_comments = comments_dataset.filter(lambda example:not example[\"is_toxic\"])[\"train\"][\"text\"]\n",
    "\n",
    "def build_system_prompt():\n",
    "    # Tune this to get more examples in the context, make sure context is wide enough to hold everything\n",
    "    N_EXAMPLES_PER_LABEL = 5\n",
    "\n",
    "    few_shots_examples = \"\"\n",
    "    # [Exercise] How to better sample toxic and non toxic comments here?\n",
    "    for comment in toxic_comments[:N_EXAMPLES_PER_LABEL]:\n",
    "        few_shots_examples += f\"- '{comment}' -> is toxic. \\n\"\n",
    "    for comment in non_toxic_comments[:N_EXAMPLES_PER_LABEL]:\n",
    "        few_shots_examples += f\"- '{comment}' -> is NOT toxic.\\n\"\n",
    "    \n",
    "    prefix = \"You are a helpful expert that decides whether a comment is toxic or not for moderation purposes on a social media platform.\"\n",
    "    prefix += \"The definition of 'toxic comment' here would be: 'a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective'\\n\"\n",
    "    prefix += \"Please always answer by saying Yes only or No only, nothing else. \\n\"\n",
    "    prefix += \"Here is a bullet point list of a few examples with their respective labels:\\n\"\n",
    "    \n",
    "    suffix = \"That's it, please do good work!\"\n",
    "\n",
    "    full_system_prompt = f\"{prefix} {few_shots_examples} \\n\\n  {suffix}\"\n",
    "\n",
    "    return full_system_prompt\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = build_system_prompt()\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "def get_model_opinion(model, comment):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Is the following comment in quotes toxic? '{comment}'\"\n",
    "            }\n",
    "        ],\n",
    "        n=1,\n",
    "        max_completion_tokens=1,\n",
    "    )\n",
    "    # Optionally, retrieve logprobs too for finer optimisations.\n",
    "\n",
    "    response = completion.choices[0].message.content\n",
    "    return {\n",
    "        \"prediction\": \"yes\" in response.lower(), # /!\\ returns False for anything but 'yes' answers (even random strings)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_opinion(MODEL, \"I hate you, you're the worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0b78cd9e9a4db9a1ec2102dc5fbf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5612302338fe41fbbdd917a2715c5449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0df393acaf425fbe5aa3c1f7d0cdc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# These 600 predictions with the prompt defined above cost ~0.10$, five times more it cost for the Zero-shot notebook\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda comment_data: get_model_opinion(MODEL, comment_data[\"text\"]),\n",
    "    keep_in_memory=True,\n",
    "    load_from_cache_file=False,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.755,\n",
       " 'f1': 0.44943820224719094,\n",
       " 'precision': 0.29850746268656714,\n",
       " 'recall': 0.9090909090909091}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"validation\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"validation\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7,\n",
       " 'f1': 0.26829268292682923,\n",
       " 'precision': 0.15714285714285714,\n",
       " 'recall': 0.9166666666666666}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(\n",
    "    references=comments_dataset[\"test\"][\"is_toxic\"],\n",
    "    predictions=comments_dataset[\"test\"][\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
